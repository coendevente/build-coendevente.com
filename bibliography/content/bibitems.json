{"vent21a": {"author": [["coen", "de", "Vente", ""], ["cristina", "", "González-Gonzalo", ""], ["eric f.", "", "Thee", ""], ["mark", "van", "Grinsven", ""], ["caroline c. w.", "", "Klaver", ""], ["clara i.", "", "Sánchez", ""]], "booktitle": "Association for Research in Vision and Ophthalmology", "url": "https://iovs.arvojournals.org/article.aspx?articleid=2775505", "title": "Making AI Transferable Across OCT Scanners from Different Vendors", "abstract": "\n\nPurpose: Deep neural networks (DNNs) for optical coherence tomography (OCT) classification have been proven to work well on images from scanners that were used during training. However, since the appearance of OCT scans can differ greatly between vendors, these DNNs often fail when they are applied to scans from different manufacturers. We propose a DNN architecture for age-related macular degeneration (AMD) grading that maintains performance on OCTs from vendors not included during training.\n\nMethods: 2,598 and 680 Heidelberg Spectralis OCT scans from the European Genetic Database were used for development and testing, respectively. We tested transferability with 339 AMD-enriched Topcon OCTs from the Rotterdam Study. AMD severity classification was determined manually in accordance with the Cologne Image Reading Center and Laboratory and Rotterdam Classification, respectively. Classifications were harmonized for the evaluation of the DNNs. The proposed DNN considers each B-scan separately using a 2D ResNet-18, and internally combines the intermediate outputs related to each B-scan using a multiple instance learning approach. Even though the proposed DNN provides both B-scan level and OCT-volume level decisions, the architecture is trained end-to-end using only full volume gradings. This specific architecture makes our method robust to the variability of scanning protocols across vendors, as it is invariant to B-scan spacing. We compare this approach to a baseline that classifies the full OCT scan directly using a 3D ResNet-18.\n\nResults: The quadratic weighted kappa (QWK) for the baseline method dropped from 0.852 on the Heidelberg Spectralis dataset to 0.523 on the Topcon dataset. This QWK drop was smaller (p = 0.001) for our approach, which dropped from 0.849 to 0.717. The difference in area under the Receiver Operating Characteristic (AUC) drop was also smaller (p < 0.001) for our approach (0.969 to 0.906, -6.5%) than for the baseline method (0.971 to 0.806, -17.0%).\n\nConclusions: We present a DNN for AMD classification on OCT scans that transfers well to scans from vendors that were not used for development. This alleviates the need for retraining on data from these scanner types, which is an expensive process in terms of data acquisition, model development, and human annotation time. Furthermore, this increases the applicability of AI for OCT classification in broader scopes than the settings in which they were developed.\n\n", "optnote": "DIAG, RADIOLOGY", "year": 2021, "scholar_id": "504386056410585983", "type": "conference", "authors": "C. de Vente, C. González-Gonzalo, E. Thee, M. van Grinsven, C. Klaver and C. Sánchez", "coverpng": "Vent21a.png", "pmidnumber": -1, "url_type": "Url", "pubinfo": "2021", "cover_exists": "False"}, "vent21": {"author": [["coen", "de", "Vente", ""], ["luuk h.", "", "Boulogne", ""], ["kiran vaidhya", "", "Venkadesh", ""], ["cheryl", "", "Sital", ""], ["nikolas", "", "Lessmann", ""], ["colin", "", "Jacobs", ""], ["clara i.", "", "Sánchez", ""], ["bram", "van", "Ginneken", ""]], "title": "Automated COVID-19 Grading with Convolutional Neural Networks in Computed Tomography Scans: A Systematic Comparison", "journal": "IEEE Transactions on Artificial Intelligence", "year": 2022, "volume": "3", "number": "2", "pages": "129-138", "doi": "https://doi.org/10.1109/TAI.2021.3115093", "abstract": "\n\nAmidst the ongoing pandemic, the assessment of computed tomography (CT) images for COVID-19 presence can exceed the workload capacity of radiologists. Several studies addressed this issue by automating COVID-19 classification and grading from CT images with convolutional neural networks (CNNs). Many of these studies reported initial results of algorithms that were assembled from commonly used components. However, the choice of the components of these algorithms was often pragmatic rather than systematic and systems were not compared to each other across papers in a fair manner. We systematically investigated the effectiveness of using 3D CNNs instead of 2D CNNs for seven commonly used architectures, including DenseNet, Inception, and ResNet variants. For the architecture that performed best, we furthermore investigated the effect of initializing the network with pre-trained weights, providing automatically computed lesion maps as additional network input, and predicting a continuous instead of a categorical output. A 3D DenseNet-201 with these components achieved an area under the receiver operating characteristic curve (AUC) of 0.930 on our test set of 105 CT scans and an AUC of 0.919 on a publicly available set of 742 CT scans, a substantial improvement in comparison with a previously published 2D CNN. output contributed the least to improving the model performance. This paper provides insights into the performance benefits of various components for COVID-19 classification and grading systems. We have created a challenge on grand-challenge.org to allow for a fair comparison between the results of this and future research.\n\n", "file": "Vent21.pdf:pdf\\\\Vent21.pdf:PDF", "optnote": "DIAG, RADIOLOGY", "scholar_id": "6795267621829470688,3751302600748129411,11993509375043963622", "type": "article", "authors": "C. de Vente, L. Boulogne, K. Venkadesh, C. Sital, N. Lessmann, C. Jacobs, C. Sánchez and B. van Ginneken", "coverpng": "Vent21.png", "pmidnumber": -1, "pubinfo": "2022;3(2):129-138", "cover_exists": "False"}, "vent20": {"author": [["coen", "de", "Vente", ""], ["mark", "van", "Grinsven", ""], ["sandro", "", "De  Zanet", ""], ["agata", "", "Mosinska", ""], ["raphael", "", "Sznitman", ""], ["caroline", "", "Klaver", ""], ["clara i.", "", "Sánchez", ""]], "booktitle": "Association for Research in Vision and Ophthalmology", "title": "Estimating Uncertainty of Deep Neural Networks for Age-related Macular Degeneration Grading using Optical Coherence Tomography", "url": "https://iovs.arvojournals.org/article.aspx?articleid=2769262", "abstract": "\n\nPurpose: Deep convolutional neural networks (CNNs) are increasingly being used for eye disease screening and diagnosis. Especially the best performing variants, however, are generally overconfident in their predictions. For usefulness in clinical practice and increasing clinicians' trust on the estimated diagnosis, well-calibrated uncertainty estimates are necessary. We present a method for providing confidence scores of CNNs for age-related macular degeneration (AMD) grading in optical coherence tomography (OCT).\n\nMethods: 1,264 OCT volumes from 633 patients from the European Genetic Database (EUGENDA) were graded as one of five stages of AMD (No AMD, Early AMD, Intermediate AMD, Advanced AMD: GA, and Advanced AMD: CNV). Ten different 3D DenseNet-121 models that take a full OCT volume as input were used to predict the corresponding AMD stage. These networks were all trained on the same dataset. However, each of these networks were initialized differently. The class with the maximum average softmax output of these models was used as the final prediction. The confidence measure was the normalized average softmax output for that class.\n\nResults: The algorithm achieved an area under the Receiver Operating Characteristic of 0.9785 and a quadratic-weighted kappa score of 0.8935. The mean uncertainty, calculated as 1 - the mean confidence score, for incorrect predictions was 1.9 times as high as the mean uncertainty for correct predictions. When only using the probability output of a single network, this ratio was 1.4. Another measure for uncertainty estimation performance is the Expected Calibration Error (ECE), where a lower value is better. When comparing the method to the probability output of a single network, the ECE improved from 0.0971 to 0.0324. Figure 1 shows examples of both confident and unconfident predictions.\n\nConclusions: We present a method for improving uncertainty estimation for AMD grading in OCT, by combining the output of multiple individually trained CNNs. This increased reliability of system confidences can contribute to building trust in CNNs for retinal disease screening. Furthermore, this technique is a first step towards selective prediction in retinal disease screening, where only cases with high uncertainty predictions need to be referred for expert evaluation.\n\n", "optnote": "DIAG, RADIOLOGY", "year": 2020, "month": "6", "scholar_id": "13938730969248371423", "type": "conference", "authors": "C. de Vente, M. van Grinsven, S. De  Zanet, A. Mosinska, R. Sznitman, C. Klaver and C. Sánchez", "coverpng": "Vent20.png", "pmidnumber": -1, "url_type": "Url", "pubinfo": "2020", "cover_exists": "False"}, "less20": {"author": [["nikolas", "", "Lessmann", ""], ["clara i.", "", "Sánchez", ""], ["ludo", "", "Beenen", ""], ["luuk h.", "", "Boulogne", ""], ["monique", "", "Brink", ""], ["erdi", "", "Calli", ""], ["jean-paul", "", "Charbonnier", ""], ["ton", "", "Dofferhoff", ""], ["wouter m.", "van", "Everdingen", ""], ["paul k.", "", "Gerke", ""], ["bram", "", "Geurts", ""], ["hester a.", "", "Gietema", ""], ["miriam", "", "Groeneveld", ""], ["louis", "van", "Harten", ""], ["nils", "", "Hendrix", ""], ["ward", "", "Hendrix", ""], ["henkjan j.", "", "Huisman", ""], ["ivana", "", "Isgum", ""], ["colin", "", "Jacobs", ""], ["ruben", "", "Kluge", ""], ["michel", "", "Kok", ""], ["jasenko", "", "Krdzalic", ""], ["bianca", "", "Lassen-Schmidt", ""], ["kicky", "van", "Leeuwen", ""], ["james", "", "Meakin", ""], ["mike", "", "Overkamp", ""], ["tjalco", "van", "Rees  Vellinga", ""], ["eva m.", "van", "Rikxoort", ""], ["riccardo", "", "Samperna", ""], ["cornelia", "", "Schaefer-Prokop", ""], ["steven", "", "Schalekamp", ""], ["ernst th", "", "Scholten", ""], ["cheryl", "", "Sital", ""], ["lauran", "", "Stöger", ""], ["jonas", "", "Teuwen", ""], ["kiran", "", "Vaidhya  Venkadesh", ""], ["coen", "de", "Vente", ""], ["marieke", "", "Vermaat", ""], ["weiyi", "", "Xie", ""], ["bram", "de", "Wilde", ""], ["mathias", "", "Prokop", ""], ["bram", "van", "Ginneken", ""]], "title": "Automated Assessment of COVID-19 Reporting and Data System and Chest CT Severity Scores in Patients Suspected of Having COVID-19 Using Artificial Intelligence", "journal": "Radiology", "year": 2021, "volume": "298", "number": "1", "pages": "E18--E28", "file": ":pdf/Less20.pdf:PDF", "doi": "https://doi.org/10.1148/radiol.2020202439", "pmid": "http://www.ncbi.nlm.nih.gov/pubmed/32729810", "algorithm": "https://grand-challenge.org/algorithms/corads-ai/", "abstract": "\n\nThe coronavirus disease 2019 (COVID-19) pandemic has spread across the globe with alarming speed, morbidity, and mortality. Immediate triage of patients with chest infections suspected to be caused by COVID-19 using chest CT may be of assistance when results from definitive viral testing are delayed. Purpose: To develop and validate an artificial intelligence (AI) system to score the likelihood and extent of pulmonary COVID-19 on chest CT scans using the COVID-19 Reporting and Data System (CO-RADS) and CT severity scoring systems. Materials and Methods: The CO-RADS AI system consists of three deep-learning algorithms that automatically segment the five pulmonary lobes, assign a CO-RADS score for the suspicion of COVID-19, and assign a CT severity score for the degree of parenchymal involvement per lobe. This study retrospectively included patients who underwent a nonenhanced chest CT examination because of clinical suspicion of COVID-19 at two medical centers. The system was trained, validated, and tested with data from one of the centers. Data from the second center served as an external test set. Diagnostic performance and agreement with scores assigned by eight independent observers were measured using receiver operating characteristic analysis, linearly weighted κ values, and classification accuracy. Results: A total of 105 patients (mean age, 62 years ± 16 [standard deviation]; 61 men) and 262 patients (mean age, 64 years ± 16; 154 men) were evaluated in the internal and external test sets, respectively. The system discriminated between patients with COVID-19 and those without COVID-19, with areas under the receiver operating characteristic curve of 0.95 (95% CI: 0.91, 0.98) and 0.88 (95% CI: 0.84, 0.93), for the internal and external test sets, respectively. Agreement with the eight human observers was moderate to substantial, with mean linearly weighted κ values of 0.60 ± 0.01 for CO-RADS scores and 0.54 ± 0.01 for CT severity scores. Conclusion: With high diagnostic performance, the CO-RADS AI system correctly identified patients with COVID-19 using chest CT scans and assigned standardized CO-RADS and CT severity scores that demonstrated good agreement with findings from eight independent observers and generalized well to external data.\n\n", "optnote": "DIAG, RADIOLOGY", "scholar_id": "13623399645587834950,2370993400684076548", "type": "article", "authors": "N. Lessmann, C. Sánchez, L. Beenen, L. Boulogne, M. Brink, E. Calli, J. Charbonnier, T. Dofferhoff, W. van Everdingen, P. Gerke, B. Geurts, H. Gietema, M. Groeneveld, L. van Harten, N. Hendrix, W. Hendrix, H. Huisman, I. Isgum, C. Jacobs, R. Kluge, M. Kok, J. Krdzalic, B. Lassen-Schmidt, K. van Leeuwen, J. Meakin, M. Overkamp, T. van Rees  Vellinga, E. van Rikxoort, R. Samperna, C. Schaefer-Prokop, S. Schalekamp, E. Scholten, C. Sital, L. Stöger, J. Teuwen, K. Vaidhya  Venkadesh, C. de Vente, M. Vermaat, W. Xie, B. de Wilde, M. Prokop and B. van Ginneken", "coverpng": "Less20.png", "pmidnumber": 32729810, "pubinfo": "2021;298(1):E18-E28", "cover_exists": "False"}, "gonz21": {"author": [["cristina", "", "González-Gonzalo", ""], ["eric f.", "", "Thee", ""], ["bart", "", "Liefers", ""], ["coen", "de", "Vente", ""], ["caroline c. w.", "", "Klaver", ""], ["clara i.", "", "Sánchez", ""]], "booktitle": "Association for Research in Vision and Ophthalmology", "url": "https://iovs.arvojournals.org/article.aspx?articleid=2773295", "title": "Hierarchical curriculum learning for robust automated detection of low-prevalence retinal disease features: application to reticular pseudodrusen", "abstract": "\n\nPurpose: The low prevalence of certain retinal disease features compromises data collection for deep neural networks (DNN) development and, consequently, the benefits of automated detection. We robustify the detection of such features in scarce data settings by exploiting hierarchical information available in the data to learn from generic to specific, low-prevalence features. We focus on reticular pseudodrusen (RPD), a hallmark of intermediate age-related macular degeneration (AMD).\n\nMethods: Color fundus images (CFI) from the AREDS dataset were used for DNN development (106,994 CFI) and testing (27,066 CFI). An external test set (RS1-6) was generated with 2,790 CFI from the Rotterdam Study. In both datasets CFI were graded from generic to specific features. This allows to establish a hierarchy of binary classification tasks with decreasing prevalence: presence of AMD findings (AREDS prevalence: 88%; RS1-6: 77%), drusen (85%; 73%), large drusen (40%; 24%), RPD (1%; 4%). We created a hierarchical curriculum and developed a DNN (HC-DNN) that learned each task sequentially. We computed its performance for RPD detection in both test sets and compared it to a baseline DNN (B-DNN) that learned to detect RPD from scratch disregarding hierarchical information. We studied their robustness across datasets, while reducing the size of data available for development (same prevalences)\n\nResults: Area under the receiver operating characteristic curve (AUC) was used to measure RPD detection performance. When large development data were available, there was no significant difference between DNNs (100% data, HC-DNN: 0.96 (95% CI, 0.94-0.97) in AREDS, 0.82 (0.78-0.86) in RS1-6; B-DNN: 0.95 (0.94-0.96) in AREDS, 0.83 (0.79-0.87) in RS1-6). However, HC-DNN achieved better performance and robustness across datasets when development data were highly reduced (<50% data, p-values<0.05) (1% data, HC-DNN: 0.63 (0.60-0.66) in AREDS, 0.76 (0.72-0.80) in RS1-6; B-DNN: 0.53 (0.49-0.56) in AREDS, 0.48 (0.42-0.53) in RS1-6).\n\nConclusions: Hierarchical curriculum learning allows for knowledge transfer from general, higher-prevalence features and becomes beneficial for the detection of low-prevalence retinal features, such as RPD, in scarce data settings. Moreover, exploiting hierarchical information improves DNN robustness across datasets.\n\n", "optnote": "DIAG, RADIOLOGY", "year": 2021, "scholar_id": "8274882984481634472", "type": "conference", "authors": "C. González-Gonzalo, E. Thee, B. Liefers, C. de Vente, C. Klaver and C. Sánchez", "coverpng": "Gonz21.png", "pmidnumber": -1, "url_type": "Url", "pubinfo": "2021", "cover_exists": "False"}, "ardu20": {"author": [["alessandro", "", "Ardu", ""], ["bart", "", "Liefers", ""], ["coen", "de", "Vente", ""], ["cristina", "", "González-Gonzalo", ""], ["caroline", "", "Klaver", ""], ["clara i.", "", "Sánchez", ""]], "booktitle": "European Society of Retina Specialists", "title": "Artificial Intelligence for the Classification and Quantification of Reticular Pseudodrusen in Multimodal Retinal Images", "abstract": "\n\nPurpose:\n\nReticular pseudodrusen (RPD) are retinal lesions highly correlated with the risk of developing end-stage age-related macular degeneration (AMD) and, therefore, relevant biomarkers for understanding the progression of AMD. Due to the subtle features characterizing RPD, multiple imaging modalities are often necessary to confirm the presence and extension of RPD, considerably increasing the workload of the expert graders. We propose a deep neural network (DNN) architecture that classifies and quantifies RPD using multimodal retinal images.\n\nSetting:\n\nA cross-sectional study that compares the performance of three expert graders with a DNN trained for identifying and quantifying RPD. Conducted on retinal images drawn from the Rotterdam Study, a population-based cohort, in three modalities: color fundus photographs (CFP), fundus autofluorescence images (FAF) and near-infrared reflectance images (NIR).\n\nMethods:\n\nMultimodal images of 278 eyes of 230 patients were retrieved from the Rotterdam Study database. Of those, 72 eyes showed presence of RPD, 108 had soft distinct/indistinct drusen, and 98 had no signs of drusen as confirmed by the Rotterdam Study graders. Delineations of the areas affected with RPD were made in consensus by two human experts using CFP and NIR images simultaneously and were used as reference standard (RS) for RPD area quantification. The data was randomly divided, patient-wise, in training (243) and test (35) sets for model development and evaluation. A DNN was developed for RPD classification and quantification. The proposed DNN is based on an encoder-decoder architecture. The model jointly inputs a set of co-registered retinal image modalities (CFP, NIR, FAF) and outputs a heatmap image containing, per pixel, the likelihood of RPD presence. The 99th percentile of the values contained in this heatmap measures the likelihood of RPD presence. Three independent graders manually delineated RPD in all eyes of the test set based on the CFP and NIR and their performance was compared with the DNN in the tasks of RPD classification and quantification.\n\nResults:\n\nThe proposed DNN obtained an area under the receiver operating characteristic curve (AUROC) with 95% confidence interval (CI) of 0.939[0.818-1.0], a sensitivity (SE) of 0.928 and specificity (SP) of 0.809 for the detection of RPD in multimodal imaging. For RPD quantification, the DNN achieved a mean Dice coefficient (DSC) of 0.632+-0.261 and an intra-class correlation (ICC) of 0.676[0.294-0.999]. Comparably, for RPD classification, grader 1 obtained SE/SP pairs of 1.0/0.785, grader 2 of 1.0/0.5 and grader 3 of 1.0/0.785. For RPD quantification, the graders obtained mean DSC of 0.619+-0.196, 0.573+-0.170 and 0.697+-0.157, respectively, and an ICC of 0.721[0.340-0.999], 0.597[0.288-0.999], 0.751[0.294-0.999], respectively. Of the DNN's three false negatives, none of them was correctly classified by the three graders. The model correctly classified RPD in three of the six eyes where graders disagreed and in the only eye where none of the graders found RPD. Overall, 65.1% of the area indicated as RPD by the reference was delineated by at least one grader and only 26.5% of the total was graded as RPD by all experts. The DNN only missed 23.2% of the areas that all three graders identified correctly.\n\nConclusions:\n\nThe proposed DNN showed promising capacities in the tasks of classifying and quantifying RPD lesions on multimodal retinal images. The results show that the model is able to correctly classify and quantify RPD on eyes where lesions are difficult to spot. The probabilistic output of the model allows for the classification of RPD at different levels of confidence and indicates what retinal areas are most likely affected. This is in line with the manual assessment done by the graders. To this point, the model is developed to classify and quantify RPD only on CFP, FAF and NIR. However, introducing other imaging modalities, such as OCT, might help diminish ambiguities in the classification and quantification of this abnormality. Therefore, a future direction for improving the proposed method is to include OCT scans as an additional input to the model. Automatic classification and quantification of RPD using deep learning on multimodal images will enable the automatic and accurate analysis of increasingly large amounts of data for clinical studies and will facilitate AMD screening in the elderly  by decreasing the workload of the expert graders.\n\nFinancial Disclosure:\n\nNone\n\n", "month": "9", "optnote": "DIAG, RADIOLOGY", "year": 2020, "type": "conference", "authors": "A. Ardu, B. Liefers, C. de Vente, C. González-Gonzalo, C. Klaver and C. Sánchez", "coverpng": "Ardu20.png", "pmidnumber": -1, "pubinfo": "2020", "cover_exists": "False"}, "vent20a": {"title": "Deep learning regression for prostate cancer detection and grading in bi-parametric MRI", "author": [["coen", "de", "Vente", ""], ["pieter", "", "Vos", ""], ["matin", "", "Hosseinzadeh", ""], ["josien", "", "Pluim", ""], ["mitko", "", "Veta", ""]], "journal": "IEEE Transactions on Biomedical Engineering", "volume": "68", "number": "2", "pages": "374--383", "year": 2020, "publisher": "IEEE", "pmid": "http://www.ncbi.nlm.nih.gov/pubmed/32396068", "doi": "https://doi.org/10.1109/TBME.2020.2993528", "scholar_id": "403188362461805794", "type": "article", "authors": "C. de Vente, P. Vos, M. Hosseinzadeh, J. Pluim and M. Veta", "coverpng": "Vent20a.png", "pmidnumber": 32396068, "pubinfo": "2020;68(2):374-383", "cover_exists": "False"}, "xion21": {"title": "A global benchmark of algorithms for segmenting the left atrium from late gadolinium-enhanced cardiac magnetic resonance imaging", "author": [["zhaohan", "", "Xiong", ""], ["qing", "", "Xia", ""], ["zhiqiang", "", "Hu", ""], ["ning", "", "Huang", ""], ["cheng", "", "Bian", ""], ["yefeng", "", "Zheng", ""], ["sulaiman", "", "Vesal", ""], ["nishant", "", "Ravikumar", ""], ["andreas", "", "Maier", ""], ["xin", "", "Yang", ""], ["pheng-ann", "", "Heng", ""], ["dong", "", "Ni", ""], ["caizi", "", "Li", ""], ["qianqian", "", "Tong", ""], ["weixin", "", "Si", ""], ["elodie", "", "Puybareau", ""], ["younes", "", "Khoudli", ""], ["thierry", "", "Graud", ""], ["chen", "", "Chen", ""], ["wenjia", "", "Bai", ""], ["daniel", "", "Rueckert", ""], ["lingchao", "", "Xu", ""], ["xiahai", "", "Zhuang", ""], ["xinzhe", "", "Luo", ""], ["shuman", "", "Jia", ""], ["maxime", "", "Sermesant", ""], ["yashu", "", "Liu", ""], ["kuanquan", "", "Wang", ""], ["davide", "", "Borra", ""], ["alessandro", "", "Masci", ""], ["cristiana", "", "Corsi", ""], ["coen", "de", "Vente", ""], ["mitko", "", "Veta", ""], ["rashed", "", "Karim", ""], ["chandrakanth", "", "Jayachandran  Preetha", ""], ["sandy", "", "Engelhardt", ""], ["menyun", "", "Qiao", ""], ["yuanyuan", "", "Wang", ""], ["qian", "", "Tao", ""], ["marta", "", "Nuñez-Garcia", ""], ["oscar", "", "Camara", ""], ["nicolo", "", "Savioli", ""], ["pablo", "", "Lamata", ""], ["jichao", "", "Zhao", ""]], "journal": "Medical Image Analysis", "volume": "67", "pages": "101832", "year": 2021, "publisher": "Elsevier", "pmid": "http://www.ncbi.nlm.nih.gov/pubmed/33166776", "scholar_id": "298995361484429794", "type": "article", "authors": "Z. Xiong, Q. Xia, Z. Hu, N. Huang, C. Bian, Y. Zheng, S. Vesal, N. Ravikumar, A. Maier, X. Yang, P. Heng, D. Ni, C. Li, Q. Tong, W. Si, E. Puybareau, Y. Khoudli, T. Graud, C. Chen, W. Bai, D. Rueckert, L. Xu, X. Zhuang, X. Luo, S. Jia, M. Sermesant, Y. Liu, K. Wang, D. Borra, A. Masci, C. Corsi, C. de Vente, M. Veta, R. Karim, C. Jayachandran  Preetha, S. Engelhardt, M. Qiao, Y. Wang, Q. Tao, M. Nuñez-Garcia, O. Camara, N. Savioli, P. Lamata and J. Zhao", "coverpng": "Xion21.png", "pmidnumber": 33166776, "pubinfo": "2021;67:101832", "cover_exists": "False"}, "vent18": {"title": "Convolutional neural networks for segmentation of the left atrium from gadolinium-enhancement MRI images", "author": [["coen", "de", "Vente", ""], ["mitko", "", "Veta", ""], ["orod", "", "Razeghi", ""], ["steven", "", "Niederer", ""], ["josien", "", "Pluim", ""], ["kawal", "", "Rhode", ""], ["rashed", "", "Karim", ""]], "booktitle": "International Workshop on Statistical Atlases and Computational Models of the Heart", "pages": "348--356", "year": 2018, "organization": "Springer", "scholar_id": "1263087259858021090", "type": "inproceedings", "authors": "C. de Vente, M. Veta, O. Razeghi, S. Niederer, J. Pluim, K. Rhode and R. Karim", "coverpng": "Vent18.png", "pmidnumber": -1, "pubinfo": "2018:348-356", "cover_exists": "False"}, "schw22": {"title": "A Deep Learning Framework for the Detection and Quantification of Reticular Pseudodrusen and Drusen on Optical Coherence Tomography", "author": [["roy", "", "Schwartz", ""], ["hagar", "", "Khalid", ""], ["sandra", "", "Liakopoulos", ""], ["yanling", "", "Ouyang", ""], ["coen", "de", "Vente", ""], ["cristina", "", "González-Gonzalo", ""], ["aaron y.", "", "Lee", ""], ["robyn", "", "Guymer", ""], ["emily y.", "", "Chew", ""], ["catherine", "", "Egan", ""], ["", "", "others", ""]], "journal": "Translational Vision Science & Technology", "volume": "11", "number": "12", "pages": "3--3", "year": 2022, "doi": "https://doi.org/10.1167/tvst.11.12.3", "publisher": "The Association for Research in Vision and Ophthalmology", "scholar_id": "15646198235825211737,5612937199194471457", "type": "article", "authors": "R. Schwartz, H. Khalid, S. Liakopoulos, Y. Ouyang, C. de Vente, C. González-Gonzalo, A. Lee, R. Guymer, E. Chew, C. Egan and others", "coverpng": "Schw22.png", "pmidnumber": -1, "pubinfo": "2022;11(12):3-3", "cover_exists": "False"}, "lemi22": {"title": "Glaucomatous features in fundus photographs of eyes with 'Referable glaucoma' of a large population based labeled data set for training an Artificial Intelligence (AI) algorithm for glaucoma screening", "author": [["hans g.", "", "Lemij", ""], ["coen", "de", "Vente", ""], ["clara i.", "", "Sánchez", ""], ["jorge", "", "Cuadros", ""], ["nicolas", "", "Jaccard", ""], ["koen", "", "Vermeer", ""]], "booktitle": "Association for Research in Vision and Ophthalmology", "volume": "63", "number": "7", "url": "https://iovs.arvojournals.org/article.aspx?articleid=2782322", "pages": "2041--A0482", "year": 2022, "publisher": "The Association for Research in Vision and Ophthalmology", "scholar_id": "14483770842850181670", "type": "conference", "authors": "H. Lemij, C. de Vente, C. Sánchez, J. Cuadros, N. Jaccard and K. Vermeer", "coverpng": "Lemi22.png", "pmidnumber": -1, "url_type": "Url", "pubinfo": "2022;63(7):2041-A0482", "cover_exists": "False"}, "schw22a": {"title": "A deep learning pipeline for the detection and quantification of drusen and reticular pseudodrusen on optical coherence tomography", "author": [["roy", "", "Schwartz", ""], ["hagar", "", "Khalid", ""], ["sandra", "", "Liakopoulos", ""], ["yanling", "", "Ouyang", ""], ["coen", "de", "Vente", ""], ["cristina gonzález", "", "Gonzalo", ""], ["aaron y.", "", "Lee", ""], ["catherine a.", "", "Egan", ""], ["clara i.", "", "Sánchez", ""], ["adnan", "", "Tufail", ""]], "booktitle": "Association for Research in Vision and Ophthalmology", "url": "https://iovs.arvojournals.org/article.aspx?articleid=2781366", "volume": "63", "number": "7", "pages": "3856--3856", "year": 2022, "publisher": "The Association for Research in Vision and Ophthalmology", "scholar_id": "6469598480820735470", "type": "conference", "authors": "R. Schwartz, H. Khalid, S. Liakopoulos, Y. Ouyang, C. de Vente, C. Gonzalo, A. Lee, C. Egan, C. Sánchez and A. Tufail", "coverpng": "Schw22a.png", "pmidnumber": -1, "url_type": "Url", "pubinfo": "2022;63(7):3856-3856", "cover_exists": "False"}, "vent22": {"author": [["coen", "de", "Vente", ""], ["koen", "", "Vermeer", ""], ["nicolas", "", "Jaccard", ""], ["hans g.", "", "Lemij", ""], ["clara i.", "", "Sánchez", ""]], "booktitle": "Imaging and Morphometry Association for Glaucoma in Europe", "url": "https://drive.google.com/file/d/11DfeyNA8I4UVZGkhn_NVIOIGbcEesfCw/view?usp=sharing", "title": "AIROGS: Artificial Intelligence for RObust Glaucoma Screening Challenge", "abstract": "\n\nPurpose: Glaucoma is a leading cause of irreversible blindness and impaired vision, which can be prevented in many cases by early detection. Artificial intelligence (AI) solutions for glaucoma screening from color fundus photographs (CFPs) have been demonstrated, but their performance often drops when they are applied in real-world settings. We present a challenge aiming at accelerating the development of robust AI models for automated glaucoma screening by CFP.\n\nMethods: 112,732 CFPs from 60,071 subjects from a population screening program for diabetic retinopathy, obtained from EyePACS, California, USA, were manually labeled by 20 carefully selected and continuously monitored ophthalmologists and optometrists. They each labeled a portion of the full set of images as “referable glaucoma” (RG), “no referable glaucoma” (NRG) or “ungradable” (U). Each CFP was graded by 2 randomly selected graders; if their labels matched, it was considered the final label. In case of disagreement, the CFP was graded by a glaucoma specialist; his label was the final label. We split the data into a development set of 101,442 CFPs and a test set of 11,290 CFPs. The challenge task was to classify CFPs as RG or NRG, while additionally providing a decision on whether images were ungradable. To encourage the development of methodologies with inherent robustness mechanisms, we only included CFPs labeled as U in the test set and not in the development set. Challenge participants submitted their solutions as Docker1 containers to our online evaluation platform2. We ran their submitted algorithms on our test set, which is not publicly available. Subsequently, we assessed glaucoma screening performance using the partial area under the receiver operator characteristic curve (pAUC) (90-100% specificity) and sensitivity at 95% specificity (S). To measure robustness, we calculated Cohen's kappa score (κU) and the area under the receiver operator characteristic curve (AUCU), using the decisions generated by the algorithms on image ungradability.\n\nResults: The challenge is currently running and we are still accepting submissions at the time of writing. Up to now, 289 users have joined the challenge, 208 persons have requested access to the dataset, 26 teams have been formed on the challenge platform, and 13 submissions from 7 unique participants have been successfully submitted to the last preliminary test set, which contains 10% of the test data. The best pAUC, S, κU and AUCU on this preliminary test set were 89.1%, 83.8%, 44.5% and 91.5%, respectively. The means and standard deviations for these metrics over all submissions were 82.2% ± 9.6%, 72.0% ± 19.4%, 20.6% ± 14.9%, 76.8% ± 11.6%, respectively.\n\nConclusions: We present a challenge based on real-world data for glaucoma screening by CFP. The initial results are promising, as the performances are high and the preliminary sensitivity at 95% specificity exceeds our target of 80%. The final winners and their solutions will be presented at the 19th IMAGE meeting.\n\n", "optnote": "DIAG, RADIOLOGY", "year": 2022, "type": "conference", "authors": "C. de Vente, K. Vermeer, N. Jaccard, H. Lemij and C. Sánchez", "coverpng": "Vent22.png", "pmidnumber": -1, "url_type": "Url", "pubinfo": "2022", "cover_exists": "False"}, "vent23a": {"title": "AIROGS: Artificial Intelligence for RObust Glaucoma Screening Challenge", "author": [["coen", "de", "Vente", ""], ["koenraad a.", "", "Vermeer", ""], ["nicolas", "", "Jaccard", ""], ["he", "", "Wang", ""], ["hongyi", "", "Sun", ""], ["firas", "", "Khader", ""], ["daniel", "", "Truhn", ""], ["temirgali", "", "Aimyshev", ""], ["yerkebulan", "", "Zhanibekuly", ""], ["tien-dung", "", "Le", ""], ["adrian", "", "Galdran", ""], ["miguel ángel", "", "González  Ballester", ""], ["gustavo", "", "Carneiro", ""], ["devika r.", "", "G", ""], ["hrishikesh p.", "", "S", ""], ["densen", "", "Puthussery", ""], ["hong", "", "Liu", ""], ["zekang", "", "Yang", ""], ["satoshi", "", "Kondo", ""], ["satoshi", "", "Kasai", ""], ["edward", "", "Wang", ""], ["ashritha", "", "Durvasula", ""], ["jónathan", "", "Heras", ""], ["miguel ángel", "", "Zapata", ""], ["teresa", "", "Araújo", ""], ["guilherme", "", "Aresta", ""], ["hrvoje", "", "Bogunović", ""], ["mustafa", "", "Arikan", ""], ["yeong chan", "", "Lee", ""], ["hyun bin", "", "Cho", ""], ["yoon ho", "", "Choi", ""], ["abdul", "", "Qayyum", ""], ["imran", "", "Razzak", ""], ["bram", "van", "Ginneken", ""], ["hans g.", "", "Lemij", ""], ["clara i.", "", "Sánchez", ""]], "journal": "arXiv:2302.01738", "year": 2023, "scholar_id": "16965053761187170060,11379689467925567962,10290591934597328501,12629941101464488854", "doi": "https://doi.org/10.48550/arXiv.2302.01738", "type": "preprint", "authors": "C. de Vente, K. Vermeer, N. Jaccard, H. Wang, H. Sun, F. Khader, D. Truhn, T. Aimyshev, Y. Zhanibekuly, T. Le, A. Galdran, M. González  Ballester, G. Carneiro, D. G, H. S, D. Puthussery, H. Liu, Z. Yang, S. Kondo, S. Kasai, E. Wang, A. Durvasula, J. Heras, M. Zapata, T. Araújo, G. Aresta, H. Bogunović, M. Arikan, Y. Lee, H. Cho, Y. Choi, A. Qayyum, I. Razzak, B. van Ginneken, H. Lemij and C. Sánchez", "coverpng": "Vent23a.png", "pmidnumber": -1, "url": "https://arxiv.org/abs/2302.01738", "url_type": "arXiv", "pubinfo": "2023", "cover_exists": "False"}, "vent23b": {"title": "Uncertainty-Aware Multiple-Instance Learning for Reliable Classification: Application to Optical Coherence Tomography", "author": [["coen", "de", "Vente", ""], ["bram", "van", "Ginneken", ""], ["carel b.", "", "Hoyng", ""], ["caroline cw", "", "Klaver", ""], ["clara i.", "", "Sánchez", ""]], "journal": "arXiv:2302.03116", "year": 2023, "doi": "https://doi.org/10.48550/arXiv.2302.03116", "type": "preprint", "authors": "C. de Vente, B. van Ginneken, C. Hoyng, C. Klaver and C. Sánchez", "coverpng": "Vent23b.png", "pmidnumber": -1, "url": "https://arxiv.org/abs/2302.03116", "url_type": "arXiv", "pubinfo": "2023", "cover_exists": "False"}, "lemi23": {"title": "Characteristics of a large, labeled dataset for the training of artificial intelligence for glaucoma screening with fundus photographs", "author": [["hans g.", "", "Lemij", ""], ["coen", "de", "Vente", ""], ["clara i.", "", "Sánchez", ""], ["koen a.", "", "Vermeer", ""]], "journal": "Ophthalmology Science", "pages": "100300", "year": 2023, "publisher": "Elsevier", "doi": "https://doi.org/10.1016/j.xops.2023.100300", "type": "article", "authors": "H. Lemij, C. de Vente, C. Sánchez and K. Vermeer", "coverpng": "Lemi23.png", "pmidnumber": -1, "pubinfo": "2023:100300", "cover_exists": "False"}, "vent23c": {"author": [["coen", "de", "Vente", ""], ["a.", "", "Tufail", ""], ["s.", "", "Schmitz-Valckenberg", ""], ["m.", "", "Saßmannshausen", ""], ["c.", "", "Hoyng", ""], ["clara i.", "on behalf of the", "MACUSTAR Sánchez consortium", ""]], "title": "OCT Super-Resolution for Data Standardization using AI: A MACUSTAR report", "abstract": "\n\nPurpose:  In optical coherence tomography scans (OCTs) from multicenter studies, there is often large variability in image quality and resolution between scans. This impairs the consistency of biomarker quantification within studies, but also between different datasets. The aim of this study was to validate a super-resolution approach based on artificial intelligence (AI) for improving the resolution of OCTs (by increasing the density of the scan pattern) to consistently enhance data within studies to high-quality standards.\n\nMethods:  The MACUSTAR cohort, a European multicentre study, was used as a training set with 743 OCTs from 181 patients and validation set with 26 OCTs from 26 patients (n=3 no AMD, n=2 early AMD, n=18 intermediate AMD, n=3 late AMD). All scans were Heidelberg Spectralis OCTs with 241 B-scans. We trained a 3D diffusion model to generate high-resolution OCTs, which was used during evaluation to produce OCTs with 241 B-scans from OCTs with 120 B-scans. The performance was calculated using the mean squared error (MSE) on OCT volume-level between the generated B-scans and the original B-scans.\n\nResults: The MSE between the generated B-scans from the low-resolution OCTs and the original B-scans from the high-resolution OCTs was 0.006 ± 0.004 (mean ± SD). Fig. 1 shows visual examples of the generated OCTs compared to the original B-scans in the validation set.\n\nConclusions:  We showed the feasibility of the proposed approach to generate super-resolution OCTs, which is one of the required steps to standardize high-quality OCTs within multicenter studies. In extensions of this approach, coherence between the OCT and other modalities, such as en face imaging and other metadata, could be introduced, allowing the AI model to make better informed generative decisions.\n\n", "optnote": "DIAG, RADIOLOGY", "year": 2023, "booktitle": "Association for Research in Vision and Ophthalmology", "journal": "Investigative Ophthalmology & Visual Science", "volume": "64", "number": "8", "pages": "313--313", "publisher": "The Association for Research in Vision and Ophthalmology", "url": "https://iovs.arvojournals.org/article.aspx?articleid=2789903", "type": "conference", "authors": "C. de Vente, A. Tufail, S. Schmitz-Valckenberg, M. Saßmannshausen, C. Hoyng and C. on behalf of the MACUSTAR Sánchez consortium", "coverpng": "Vent23c.png", "pmidnumber": -1, "url_type": "Url", "pubinfo": "2023;64(8):313-313", "cover_exists": "False"}}