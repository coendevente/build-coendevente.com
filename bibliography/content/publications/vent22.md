title: AIROGS: Artificial Intelligence for RObust Glaucoma Screening Challenge
authors: C. de Vente, K. Vermeer, N. Jaccard, H.G. Lemij and C.I. Sánchez
has_pdf: False
template: publication
bibkey: vent22
published_in: Imaging and Morphometry Association for Glaucoma in Europe
pub_details: in: <i>Imaging and Morphometry Association for Glaucoma in Europe</i>, 2022
urlweb: https://drive.google.com/file/d/11DfeyNA8I4UVZGkhn_NVIOIGbcEesfCw/view?usp=sharing
Purpose Glaucoma is a leading cause of irreversible blindness and impaired vision, which can be prevented in many cases by early detection. Artificial intelligence (AI) solutions for glaucoma screening from color fundus photographs (CFPs) have been demonstrated, but their performance often drops when they are applied in real-world settings. We present a challenge aiming at accelerating the development of robust AI models for automated glaucoma screening by CFP.  Methods: 112,732 CFPs from 60,071 subjects from a population screening program for diabetic retinopathy, obtained from EyePACS, California, USA, were manually labeled by 20 carefully selected and continuously monitored ophthalmologists and optometrists. They each labeled a portion of the full set of images as “referable glaucoma” (RG), “no referable glaucoma” (NRG) or “ungradable” (U). Each CFP was graded by 2 randomly selected graders; if their labels matched, it was considered the final label. In case of disagreement, the CFP was graded by a glaucoma specialist; his label was the final label. We split the data into a development set of 101,442 CFPs and a test set of 11,290 CFPs. The challenge task was to classify CFPs as RG or NRG, while additionally providing a decision on whether images were ungradable. To encourage the development of methodologies with inherent robustness mechanisms, we only included CFPs labeled as U in the test set and not in the development set. Challenge participants submitted their solutions as Docker1 containers to our online evaluation platform2. We ran their submitted algorithms on our test set, which is not publicly available. Subsequently, we assessed glaucoma screening performance using the partial area under the receiver operator characteristic curve (pAUC) (90-100% specificity) and sensitivity at 95% specificity (S). To measure robustness, we calculated Cohen's kappa score (κU) and the area under the receiver operator characteristic curve (AUCU), using the decisions generated by the algorithms on image ungradability.  Results: The challenge is currently running and we are still accepting submissions at the time of writing. Up to now, 289 users have joined the challenge, 208 persons have requested access to the dataset, 26 teams have been formed on the challenge platform, and 13 submissions from 7 unique participants have been successfully submitted to the last preliminary test set, which contains 10% of the test data. The best pAUC, S, κU and AUCU on this preliminary test set were 89.1%, 83.8%, 44.5% and 91.5%, respectively. The means and standard deviations for these metrics over all submissions were 82.2% ± 9.6%, 72.0% ± 19.4%, 20.6% ± 14.9%, 76.8% ± 11.6%, respectively.  Conclusions: We present a challenge based on real-world data for glaucoma screening by CFP. The initial results are promising, as the performances are high and the preliminary sensitivity at 95% specificity exceeds our target of 80%. The final winners and their solutions will be presented at the 19th IMAGE meeting. 

