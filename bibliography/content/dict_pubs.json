{
    "vent21a": {
        "html": "<p>C. de Vente, C. Gonz\u00e1lez-Gonzalo, E.F. Thee, M. van Grinsven, C.C. Klaver and C.I. S\u00e1nchez. \"Making AI Transferable Across OCT Scanners from Different Vendors\", in: <i>Association for Research in Vision and Ophthalmology</i>, 2021.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-vent21a-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a><a data-ix=\"goupbox\" href=\"https://scholar.google.com/scholar?oi=bibs&hl=nl&cites=504386056410585983\" target=\"_new\" class=\"knop footerknop movewithmouse w-button publication-button\">Cited by 2</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://iovs.arvojournals.org/article.aspx?articleid=2775505\">URL</a></div>\n",
        "only_text": "C. de Vente, C. Gonz\u00e1lez-Gonzalo, E.F. Thee, M. van Grinsven, C.C. Klaver and C.I. S\u00e1nchez. \"Making AI Transferable Across OCT Scanners from Different Vendors\", in: <i>Association for Research in Vision and Ophthalmology</i>, 2021.",
        "year": 2021,
        "pub_type": "@conference",
        "pub_details": "in: <i>Association for Research in Vision and Ophthalmology</i>, 2021",
        "bibtex": "Vent21a,\n  author = {de Vente, Coen and Gonz\\'{a}lez-Gonzalo, Cristina and Thee, Eric F. and van Grinsven, Mark and Klaver, Caroline C.W. and S\\'{a}nchez, Clara I.},\n  booktitle ={Association for Research in Vision and Ophthalmology}\n  url = {https://iovs.arvojournals.org/article.aspx?articleid=2775505},\n  title = {Making AI Transferable Across OCT Scanners from Different Vendors},\n\n  Methods: 2,598 and 680 Heidelberg Spectralis OCT scans from the European Genetic Database were used for development and testing, respectively. We tested transferability with 339 AMD-enriched Topcon OCTs from the Rotterdam Study. AMD severity classification was determined manually in accordance with the Cologne Image Reading Center and Laboratory and Rotterdam Classification, respectively. Classifications were harmonized for the evaluation of the DNNs. The proposed DNN considers each B-scan separately using a 2D ResNet-18, and internally combines the intermediate outputs related to each B-scan using a multiple instance learning approach. Even though the proposed DNN provides both B-scan level and OCT-volume level decisions, the architecture is trained end-to-end using only full volume gradings. This specific architecture makes our method robust to the variability of scanning protocols across vendors, as it is invariant to B-scan spacing. We compare this approach to a baseline that classifies the full OCT scan directly using a 3D ResNet-18.\n\n  Results: The quadratic weighted kappa (QWK) for the baseline method dropped from 0.852 on the Heidelberg Spectralis dataset to 0.523 on the Topcon dataset. This QWK drop was smaller (p = 0.001) for our approach, which dropped from 0.849 to 0.717. The difference in area under the Receiver Operating Characteristic (AUC) drop was also smaller (p < 0.001) for our approach (0.969 to 0.906, -6.5%) than for the baseline method (0.971 to 0.806, -17.0%).\n\n  Conclusions: We present a DNN for AMD classification on OCT scans that transfers well to scans from vendors that were not used for development. This alleviates the need for retraining on data from these scanner types, which is an expensive process in terms of data acquisition, model development, and human annotation time. Furthermore, this increases the applicability of AI for OCT classification in broader scopes than the settings in which they were developed.},\n  year = {2021},\n  scholar_id = {504386056410585983},\n"
    },
    "vent21": {
        "html": "<p>C. de Vente, L.H. Boulogne, K. Vaidhya Venkadesh, C. Sital, N. Lessmann, C. Jacobs, C.I. S\u00e1nchez and B. van Ginneken. \"Automated COVID-19 Grading with Convolutional Neural Networks in Computed Tomography Scans: A Systematic Comparison\", <i>IEEE Transactions on Artificial Intelligence</i>, 2022;3(2):129-138.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-vent21-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a><a data-ix=\"goupbox\" href=\"https://scholar.google.com/scholar?oi=bibs&hl=nl&cites=6795267621829470688,3751302600748129411,11993509375043963622\" target=\"_new\" class=\"knop footerknop movewithmouse w-button publication-button\">Cited by 17</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://doi.org/10.1109/TAI.2021.3115093\">DOI</a></div>\n",
        "only_text": "C. de Vente, L.H. Boulogne, K. Vaidhya Venkadesh, C. Sital, N. Lessmann, C. Jacobs, C.I. S\u00e1nchez and B. van Ginneken. \"Automated COVID-19 Grading with Convolutional Neural Networks in Computed Tomography Scans: A Systematic Comparison\", <i>IEEE Transactions on Artificial Intelligence</i>, 2022;3(2):129-138.",
        "year": 2022,
        "pub_type": "@article",
        "pub_details": "<i>IEEE Transactions on Artificial Intelligence</i>, 2022;3(2):129-138",
        "bibtex": "Vent21,\n  author = {Coen de Vente and Luuk H. Boulogne and Kiran Vaidhya Venkadesh and Cheryl Sital and Nikolas Lessmann and Colin Jacobs and Clara I. S\\'{a}nchez and Bram van Ginneken},\n  title = {Automated COVID-19 Grading with Convolutional Neural Networks in Computed Tomography Scans: A Systematic Comparison},\n  journal ={IEEE Transactions on Artificial Intelligence}\n  year={2022},\n  volume={3},\n  number={2},\n  pages={129-138},\n  doi={10.1109/TAI.2021.3115093},\n  scholar_id = {6795267621829470688,3751302600748129411,11993509375043963622},\n"
    },
    "vent20": {
        "html": "<p>C. de Vente, M. van Grinsven, S. De Zanet, A. Mosinska, R. Sznitman, C. Klaver and C.I. S\u00e1nchez. \"Estimating Uncertainty of Deep Neural Networks for Age-related Macular Degeneration Grading using Optical Coherence Tomography\", in: <i>Association for Research in Vision and Ophthalmology</i>, 2020.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-vent20-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a><a data-ix=\"goupbox\" href=\"https://scholar.google.com/scholar?oi=bibs&hl=nl&cites=13938730969248371423\" target=\"_new\" class=\"knop footerknop movewithmouse w-button publication-button\">Cited by 1</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://iovs.arvojournals.org/article.aspx?articleid=2769262\">URL</a></div>\n",
        "only_text": "C. de Vente, M. van Grinsven, S. De Zanet, A. Mosinska, R. Sznitman, C. Klaver and C.I. S\u00e1nchez. \"Estimating Uncertainty of Deep Neural Networks for Age-related Macular Degeneration Grading using Optical Coherence Tomography\", in: <i>Association for Research in Vision and Ophthalmology</i>, 2020.",
        "year": 2020,
        "pub_type": "@conference",
        "pub_details": "in: <i>Association for Research in Vision and Ophthalmology</i>, 2020",
        "bibtex": "Vent20,\n  author = {de Vente, Coen and van Grinsven, Mark and De Zanet, Sandro and Mosinska, Agata and Sznitman, Raphael and Klaver, Caroline and S\\'{a}nchez, Clara I.},\n  booktitle ={Association for Research in Vision and Ophthalmology}\n  title = {Estimating Uncertainty of Deep Neural Networks for Age-related Macular Degeneration Grading using Optical Coherence Tomography},\n  url={https://iovs.arvojournals.org/article.aspx?articleid=2769262},\n\n\n                          Methods: 1,264 OCT volumes from 633 patients from the European Genetic Database (EUGENDA) were graded as one of five stages of AMD (No AMD, Early AMD, Intermediate AMD, Advanced AMD: GA, and Advanced AMD: CNV). Ten different 3D DenseNet-121 models that take a full OCT volume as input were used to predict the corresponding AMD stage. These networks were all trained on the same dataset. However, each of these networks were initialized differently. The class with the maximum average softmax output of these models was used as the final prediction. The confidence measure was the normalized average softmax output for that class.\n\n                          Results: The algorithm achieved an area under the Receiver Operating Characteristic of 0.9785 and a quadratic-weighted kappa score of 0.8935. The mean uncertainty, calculated as 1 - the mean confidence score, for incorrect predictions was 1.9 times as high as the mean uncertainty for correct predictions. When only using the probability output of a single network, this ratio was 1.4. Another measure for uncertainty estimation performance is the Expected Calibration Error (ECE), where a lower value is better. When comparing the method to the probability output of a single network, the ECE improved from 0.0971 to 0.0324. Figure 1 shows examples of both confident and unconfident predictions.\n\n                          Conclusions: We present a method for improving uncertainty estimation for AMD grading in OCT, by combining the output of multiple individually trained CNNs. This increased reliability of system confidences can contribute to building trust in CNNs for retinal disease screening. Furthermore, this technique is a first step towards selective prediction in retinal disease screening, where only cases with high uncertainty predictions need to be referred for expert evaluation.},\n  year = {2020},\n  month = {6},\n  scholar_id = {13938730969248371423},\n"
    },
    "less20": {
        "html": "<p>N. Lessmann, C.I. S\u00e1nchez, L. Beenen, L.H. Boulogne, M. Brink, E. Calli, J. Charbonnier, T. Dofferhoff, W.M. van Everdingen, P.K. Gerke, B. Geurts, H.A. Gietema, M. Groeneveld, L. van Harten, N. Hendrix, W. Hendrix, H.J. Huisman, I. Isgum, C. Jacobs, R. Kluge, M. Kok, J. Krdzalic, B. Lassen-Schmidt, K. van Leeuwen, J. Meakin, M. Overkamp, T. van Rees Vellinga, E.M. van Rikxoort, R. Samperna, C. Schaefer-Prokop, S. Schalekamp, E.T. Scholten, C. Sital, L. St\u00f6ger, J. Teuwen, K. Vaidhya Venkadesh, C. de Vente, M. Vermaat, W. Xie, B. de Wilde, M. Prokop and B. van Ginneken. \"Automated Assessment of COVID-19 Reporting and Data System and Chest CT Severity Scores in Patients Suspected of Having COVID-19 Using Artificial Intelligence\", <i>Radiology</i>, 2021;298(1):E18-E28.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-less20-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a><a data-ix=\"goupbox\" href=\"https://scholar.google.com/scholar?oi=bibs&hl=nl&cites=13623399645587834950,2370993400684076548\" target=\"_new\" class=\"knop footerknop movewithmouse w-button publication-button\">Cited by 149</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://doi.org/10.1148/radiol.2020202439\">DOI</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"http://www.ncbi.nlm.nih.gov/pubmed/32729810/\">PMID</a></div>\n",
        "only_text": "N. Lessmann, C.I. S\u00e1nchez, L. Beenen, L.H. Boulogne, M. Brink, E. Calli, J. Charbonnier, T. Dofferhoff, W.M. van Everdingen, P.K. Gerke, B. Geurts, H.A. Gietema, M. Groeneveld, L. van Harten, N. Hendrix, W. Hendrix, H.J. Huisman, I. Isgum, C. Jacobs, R. Kluge, M. Kok, J. Krdzalic, B. Lassen-Schmidt, K. van Leeuwen, J. Meakin, M. Overkamp, T. van Rees Vellinga, E.M. van Rikxoort, R. Samperna, C. Schaefer-Prokop, S. Schalekamp, E.T. Scholten, C. Sital, L. St\u00f6ger, J. Teuwen, K. Vaidhya Venkadesh, C. de Vente, M. Vermaat, W. Xie, B. de Wilde, M. Prokop and B. van Ginneken. \"Automated Assessment of COVID-19 Reporting and Data System and Chest CT Severity Scores in Patients Suspected of Having COVID-19 Using Artificial Intelligence\", <i>Radiology</i>, 2021;298(1):E18-E28.",
        "year": 2021,
        "pub_type": "@article",
        "pub_details": "<i>Radiology</i>, 2021;298(1):E18-E28",
        "bibtex": "Less20,\n  author = {Lessmann, Nikolas and S\\'{a}nchez, Clara I. and Beenen, Ludo and Boulogne, Luuk H. and Brink, Monique and Calli, Erdi and Charbonnier, Jean-Paul and Dofferhoff, Ton and van Everdingen, Wouter M. and Gerke, Paul K. and Geurts, Bram and Gietema, Hester A. and Groeneveld, Miriam and van Harten, Louis and Hendrix, Nils and Hendrix, Ward and Huisman, Henkjan J. and Isgum, Ivana and Jacobs, Colin and Kluge, Ruben and Kok, Michel and Krdzalic, Jasenko and Lassen-Schmidt, Bianca and van Leeuwen, Kicky and Meakin, James and Overkamp, Mike and van Rees Vellinga, Tjalco and van Rikxoort, Eva M. and Samperna, Riccardo and Schaefer-Prokop, Cornelia and Schalekamp, Steven and Scholten, Ernst Th. and Sital, Cheryl and St\\\"{o}ger, Lauran and Teuwen, Jonas and Vaidhya Venkadesh, Kiran and de Vente, Coen and Vermaat, Marieke and Xie, Weiyi and de Wilde, Bram and Prokop, Mathias and van Ginneken, Bram},\n  title = {Automated Assessment of {COVID}-19 Reporting and Data System and Chest {CT} Severity Scores in Patients Suspected of Having {COVID}-19 Using Artificial Intelligence},\n  journal ={Radiology}\n  year = {2021},\n  volume = {298},\n  number = {1},\n  pages = {E18--E28},\n  doi = {10.1148/radiol.2020202439},\n  pmid = {32729810},\n  algorithm = {https://grand-challenge.org/algorithms/corads-ai/},\n  scholar_id = {13623399645587834950,2370993400684076548},\n"
    },
    "gonz21": {
        "html": "<p>C. Gonz\u00e1lez-Gonzalo, E.F. Thee, B. Liefers, C. de Vente, C.C. Klaver and C.I. S\u00e1nchez. \"Hierarchical curriculum learning for robust automated detection of low-prevalence retinal disease features: application to reticular pseudodrusen\", in: <i>Association for Research in Vision and Ophthalmology</i>, 2021.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-gonz21-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a><a data-ix=\"goupbox\" href=\"https://scholar.google.com/scholar?oi=bibs&hl=nl&cites=8274882984481634472\" target=\"_new\" class=\"knop footerknop movewithmouse w-button publication-button\">Cited by 1</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://iovs.arvojournals.org/article.aspx?articleid=2773295\">URL</a></div>\n",
        "only_text": "C. Gonz\u00e1lez-Gonzalo, E.F. Thee, B. Liefers, C. de Vente, C.C. Klaver and C.I. S\u00e1nchez. \"Hierarchical curriculum learning for robust automated detection of low-prevalence retinal disease features: application to reticular pseudodrusen\", in: <i>Association for Research in Vision and Ophthalmology</i>, 2021.",
        "year": 2021,
        "pub_type": "@conference",
        "pub_details": "in: <i>Association for Research in Vision and Ophthalmology</i>, 2021",
        "bibtex": "Gonz21,\n  author = {Gonz\\'{a}lez-Gonzalo, Cristina and Thee, Eric F. and Liefers, Bart and de Vente, Coen and Klaver, Caroline C.W. and S\\'{a}nchez, Clara I.},\n  booktitle ={Association for Research in Vision and Ophthalmology}\n  url = {https://iovs.arvojournals.org/article.aspx?articleid=2773295},\n  title = {Hierarchical curriculum learning for robust automated detection of low-prevalence retinal disease features: application to reticular pseudodrusen},\n\n  Methods: Color fundus images (CFI) from the AREDS dataset were used for DNN development (106,994 CFI) and testing (27,066 CFI). An external test set (RS1-6) was generated with 2,790 CFI from the Rotterdam Study. In both datasets CFI were graded from generic to specific features. This allows to establish a hierarchy of binary classification tasks with decreasing prevalence: presence of AMD findings (AREDS prevalence: 88%; RS1-6: 77%), drusen (85%; 73%), large drusen (40%; 24%), RPD (1%; 4%). We created a hierarchical curriculum and developed a DNN (HC-DNN) that learned each task sequentially. We computed its performance for RPD detection in both test sets and compared it to a baseline DNN (B-DNN) that learned to detect RPD from scratch disregarding hierarchical information. We studied their robustness across datasets, while reducing the size of data available for development (same prevalences)\n\n  Results: Area under the receiver operating characteristic curve (AUC) was used to measure RPD detection performance. When large development data were available, there was no significant difference between DNNs (100% data, HC-DNN: 0.96 (95% CI, 0.94-0.97) in AREDS, 0.82 (0.78-0.86) in RS1-6; B-DNN: 0.95 (0.94-0.96) in AREDS, 0.83 (0.79-0.87) in RS1-6). However, HC-DNN achieved better performance and robustness across datasets when development data were highly reduced (<50% data, p-values<0.05) (1% data, HC-DNN: 0.63 (0.60-0.66) in AREDS, 0.76 (0.72-0.80) in RS1-6; B-DNN: 0.53 (0.49-0.56) in AREDS, 0.48 (0.42-0.53) in RS1-6).\n\n  Conclusions: Hierarchical curriculum learning allows for knowledge transfer from general, higher-prevalence features and becomes beneficial for the detection of low-prevalence retinal features, such as RPD, in scarce data settings. Moreover, exploiting hierarchical information improves DNN robustness across datasets.},\n  year = {2021},\n  scholar_id = {8274882984481634472},\n"
    },
    "ardu20": {
        "html": "<p>A. Ardu, B. Liefers, C. de Vente, C. Gonz\u00e1lez-Gonzalo, C. Klaver and C.I. S\u00e1nchez. \"Artificial Intelligence for the Classification and Quantification of Reticular Pseudodrusen in Multimodal Retinal Images\", in: <i>European Society of Retina Specialists</i>, 2020.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-ardu20-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a></div>\n",
        "only_text": "A. Ardu, B. Liefers, C. de Vente, C. Gonz\u00e1lez-Gonzalo, C. Klaver and C.I. S\u00e1nchez. \"Artificial Intelligence for the Classification and Quantification of Reticular Pseudodrusen in Multimodal Retinal Images\", in: <i>European Society of Retina Specialists</i>, 2020.",
        "year": 2020,
        "pub_type": "@Conference",
        "pub_details": "in: <i>European Society of Retina Specialists</i>, 2020",
        "bibtex": "Ardu20,\n  author    = {Ardu, Alessandro and Liefers, Bart and de Vente, Coen and Gonz\\'{a}lez-Gonzalo, Cristina and Klaver, Caroline and S\\'{a}nchez, Clara I.},\n  booktitle ={European Society of Retina Specialists}\n  title     = {Artificial Intelligence for the Classification and Quantification of Reticular Pseudodrusen in Multimodal Retinal Images},\n   Reticular pseudodrusen (RPD) are retinal lesions highly correlated with the risk of developing end-stage age-related macular degeneration (AMD) and, therefore, relevant biomarkers for understanding the progression of AMD. Due to the subtle features characterizing RPD, multiple imaging modalities are often necessary to confirm the presence and extension of RPD, considerably increasing the workload of the expert graders. We propose a deep neural network (DNN) architecture that classifies and quantifies RPD using multimodal retinal images.\n   Setting:\n   A cross-sectional study that compares the performance of three expert graders with a DNN trained for identifying and quantifying RPD. Conducted on retinal images drawn from the Rotterdam Study, a population-based cohort, in three modalities: color fundus photographs (CFP), fundus autofluorescence images (FAF) and near-infrared reflectance images (NIR).\n   Methods:\n   Multimodal images of 278 eyes of 230 patients were retrieved from the Rotterdam Study database. Of those, 72 eyes showed presence of RPD, 108 had soft distinct/indistinct drusen, and 98 had no signs of drusen as confirmed by the Rotterdam Study graders. Delineations of the areas affected with RPD were made in consensus by two human experts using CFP and NIR images simultaneously and were used as reference standard (RS) for RPD area quantification. The data was randomly divided, patient-wise, in training (243) and test (35) sets for model development and evaluation. A DNN was developed for RPD classification and quantification. The proposed DNN is based on an encoder-decoder architecture. The model jointly inputs a set of co-registered retinal image modalities (CFP, NIR, FAF) and outputs a heatmap image containing, per pixel, the likelihood of RPD presence. The 99th percentile of the values contained in this heatmap measures the likelihood of RPD presence. Three independent graders manually delineated RPD in all eyes of the test set based on the CFP and NIR and their performance was compared with the DNN in the tasks of RPD classification and quantification.\n   Results:\n   The proposed DNN obtained an area under the receiver operating characteristic curve (AUROC) with 95% confidence interval (CI) of 0.939[0.818-1.0], a sensitivity (SE) of 0.928 and specificity (SP) of 0.809 for the detection of RPD in multimodal imaging. For RPD quantification, the DNN achieved a mean Dice coefficient (DSC) of 0.632+-0.261 and an intra-class correlation (ICC) of 0.676[0.294-0.999]. Comparably, for RPD classification, grader 1 obtained SE/SP pairs of 1.0/0.785, grader 2 of 1.0/0.5 and grader 3 of 1.0/0.785. For RPD quantification, the graders obtained mean DSC of 0.619+-0.196, 0.573+-0.170 and 0.697+-0.157, respectively, and an ICC of 0.721[0.340-0.999], 0.597[0.288-0.999], 0.751[0.294-0.999], respectively. Of the DNN's three false negatives, none of them was correctly classified by the three graders. The model correctly classified RPD in three of the six eyes where graders disagreed and in the only eye where none of the graders found RPD. Overall, 65.1% of the area indicated as RPD by the reference was delineated by at least one grader and only 26.5% of the total was graded as RPD by all experts. The DNN only missed 23.2% of the areas that all three graders identified correctly.\n   Conclusions:\n   The proposed DNN showed promising capacities in the tasks of classifying and quantifying RPD lesions on multimodal retinal images. The results show that the model is able to correctly classify and quantify RPD on eyes where lesions are difficult to spot. The probabilistic output of the model allows for the classification of RPD at different levels of confidence and indicates what retinal areas are most likely affected. This is in line with the manual assessment done by the graders. To this point, the model is developed to classify and quantify RPD only on CFP, FAF and NIR. However, introducing other imaging modalities, such as OCT, might help diminish ambiguities in the classification and quantification of this abnormality. Therefore, a future direction for improving the proposed method is to include OCT scans as an additional input to the model. Automatic classification and quantification of RPD using deep learning on multimodal images will enable the automatic and accurate analysis of increasingly large amounts of data for clinical studies and will facilitate AMD screening in the elderly  by decreasing the workload of the expert graders.\n   Financial Disclosure:\n   None},\n  month     = {9},\n  year      = {2020},\n"
    },
    "vent20a": {
        "html": "<p>C. de Vente, P. Vos, M. Hosseinzadeh, J. Pluim and M. Veta. \"Deep learning regression for prostate cancer detection and grading in bi-parametric MRI\", <i>IEEE Transactions on Biomedical Engineering</i>, 2020;68(2):374-383.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-vent20a-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a><a data-ix=\"goupbox\" href=\"https://scholar.google.com/scholar?oi=bibs&hl=nl&cites=403188362461805794\" target=\"_new\" class=\"knop footerknop movewithmouse w-button publication-button\">Cited by 83</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://doi.org/10.1109/TBME.2020.2993528\">DOI</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"http://www.ncbi.nlm.nih.gov/pubmed/32396068/\">PMID</a></div>\n",
        "only_text": "C. de Vente, P. Vos, M. Hosseinzadeh, J. Pluim and M. Veta. \"Deep learning regression for prostate cancer detection and grading in bi-parametric MRI\", <i>IEEE Transactions on Biomedical Engineering</i>, 2020;68(2):374-383.",
        "year": 2020,
        "pub_type": "@article",
        "pub_details": "<i>IEEE Transactions on Biomedical Engineering</i>, 2020;68(2):374-383",
        "bibtex": "Vent20a,\n  title={Deep learning regression for prostate cancer detection and grading in bi-parametric MRI},\n  author={de Vente, Coen and Vos, Pieter and Hosseinzadeh, Matin and Pluim, Josien and Veta, Mitko},\n  journal={IEEE Transactions on Biomedical Engineering}\n  volume={68},\n  number={2},\n  pages={374--383},\n  year={2020},\n  publisher={IEEE},\n  pmid={32396068},\n  doi={10.1109/TBME.2020.2993528},\n  scholar_id={403188362461805794},\n"
    },
    "xion21": {
        "html": "<p>Z. Xiong, Q. Xia, Z. Hu, N. Huang, C. Bian, Y. Zheng, S. Vesal, N. Ravikumar, A. Maier, X. Yang, P. Heng, D. Ni, C. Li, Q. Tong, W. Si, E. Puybareau, Y. Khoudli, T. Graud, C. Chen, W. Bai, D. Rueckert, L. Xu, X. Zhuang, X. Luo, S. Jia, M. Sermesant, Y. Liu, K. Wang, D. Borra, A. Masci, C. Corsi, C. de Vente, M. Veta, R. Karim, C. Jayachandran Preetha, S. Engelhardt, M. Qiao, Y. Wang, Q. Tao, M. Nu\u00f1ez-Garcia, O. Camara, N. Savioli, P. Lamata and J. Zhao. \"A global benchmark of algorithms for segmenting the left atrium from late gadolinium-enhanced cardiac magnetic resonance imaging\", <i>Medical Image Analysis</i>, 2021;67:101832.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-xion21-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a><a data-ix=\"goupbox\" href=\"https://scholar.google.com/scholar?oi=bibs&hl=nl&cites=298995361484429794\" target=\"_new\" class=\"knop footerknop movewithmouse w-button publication-button\">Cited by 138</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"http://www.ncbi.nlm.nih.gov/pubmed/33166776/\">PMID</a></div>\n",
        "only_text": "Z. Xiong, Q. Xia, Z. Hu, N. Huang, C. Bian, Y. Zheng, S. Vesal, N. Ravikumar, A. Maier, X. Yang, P. Heng, D. Ni, C. Li, Q. Tong, W. Si, E. Puybareau, Y. Khoudli, T. Graud, C. Chen, W. Bai, D. Rueckert, L. Xu, X. Zhuang, X. Luo, S. Jia, M. Sermesant, Y. Liu, K. Wang, D. Borra, A. Masci, C. Corsi, C. de Vente, M. Veta, R. Karim, C. Jayachandran Preetha, S. Engelhardt, M. Qiao, Y. Wang, Q. Tao, M. Nu\u00f1ez-Garcia, O. Camara, N. Savioli, P. Lamata and J. Zhao. \"A global benchmark of algorithms for segmenting the left atrium from late gadolinium-enhanced cardiac magnetic resonance imaging\", <i>Medical Image Analysis</i>, 2021;67:101832.",
        "year": 2021,
        "pub_type": "@article",
        "pub_details": "<i>Medical Image Analysis</i>, 2021;67:101832",
        "bibtex": "Xion21,\n  title={A global benchmark of algorithms for segmenting the left atrium from late gadolinium-enhanced cardiac magnetic resonance imaging},\n  author={Xiong, Zhaohan and Xia, Qing and Hu, Zhiqiang and Huang, Ning and Bian, Cheng and Zheng, Yefeng and Vesal, Sulaiman and Ravikumar, Nishant and Maier, Andreas and Yang, Xin and Heng, Pheng-Ann and Ni, Dong and Li, Caizi and Tong, Qianqian and Si, Weixin and Puybareau, Elodie and Khoudli, Younes and Graud, Thierry and Chen, Chen and Bai, Wenjia and Rueckert, Daniel and Xu, Lingchao and Zhuang, Xiahai and Luo, Xinzhe and Jia, Shuman and Sermesant, Maxime and Liu, Yashu and Wang, Kuanquan and Borra, Davide and Masci, Alessandro and Corsi, Cristiana and de Vente, Coen and Veta, Mitko and Karim, Rashed and Jayachandran Preetha, Chandrakanth and Engelhardt, Sandy and Qiao, Menyun and Wang, Yuanyuan and Tao, Qian and Nu\u00f1ez-Garcia, Marta and Camara, Oscar and Savioli, Nicolo and Lamata, Pablo and Zhao, Jichao},\n  journal={Medical Image Analysis}\n  volume={67},\n  pages={101832},\n  year={2021},\n  publisher={Elsevier},\n  pmid={33166776},\n  scholar_id={298995361484429794},\n"
    },
    "vent18": {
        "html": "<p>C. de Vente, M. Veta, O. Razeghi, S. Niederer, J. Pluim, K. Rhode and R. Karim. \"Convolutional neural networks for segmentation of the left atrium from gadolinium-enhancement MRI images\", in: <i>International Workshop on Statistical Atlases and Computational Models of the Heart</i>, 2018, pages 348-356.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-vent18-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a><a data-ix=\"goupbox\" href=\"https://scholar.google.com/scholar?oi=bibs&hl=nl&cites=1263087259858021090\" target=\"_new\" class=\"knop footerknop movewithmouse w-button publication-button\">Cited by 9</a></div>\n",
        "only_text": "C. de Vente, M. Veta, O. Razeghi, S. Niederer, J. Pluim, K. Rhode and R. Karim. \"Convolutional neural networks for segmentation of the left atrium from gadolinium-enhancement MRI images\", in: <i>International Workshop on Statistical Atlases and Computational Models of the Heart</i>, 2018, pages 348-356.",
        "year": 2018,
        "pub_type": "@inproceedings",
        "pub_details": "in: <i>International Workshop on Statistical Atlases and Computational Models of the Heart</i>, 2018, pages 348-356",
        "bibtex": "Vent18,\n  title={Convolutional neural networks for segmentation of the left atrium from gadolinium-enhancement MRI images},\n  author={de Vente, Coen and Veta, Mitko and Razeghi, Orod and Niederer, Steven and Pluim, Josien and Rhode, Kawal and Karim, Rashed},\n  booktitle={International Workshop on Statistical Atlases and Computational Models of the Heart}\n  pages={348--356},\n  year={2018},\n  organization={Springer},\n  scholar_id={1263087259858021090},\n"
    },
    "schw22": {
        "html": "<p>R. Schwartz, H. Khalid, S. Liakopoulos, Y. Ouyang, C. de Vente, C. Gonz\u00e1lez-Gonzalo, A.Y. Lee, R. Guymer, E.Y. Chew, C. Egan and others. \"A Deep Learning Framework for the Detection and Quantification of Reticular Pseudodrusen and Drusen on Optical Coherence Tomography\", <i>Translational Vision Science & Technology</i>, 2022;11(12):3-3.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-schw22-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a><a data-ix=\"goupbox\" href=\"https://scholar.google.com/scholar?oi=bibs&hl=nl&cites=15646198235825211737,5612937199194471457\" target=\"_new\" class=\"knop footerknop movewithmouse w-button publication-button\">Cited by 4</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://doi.org/10.1167/tvst.11.12.3\">DOI</a></div>\n",
        "only_text": "R. Schwartz, H. Khalid, S. Liakopoulos, Y. Ouyang, C. de Vente, C. Gonz\u00e1lez-Gonzalo, A.Y. Lee, R. Guymer, E.Y. Chew, C. Egan and others. \"A Deep Learning Framework for the Detection and Quantification of Reticular Pseudodrusen and Drusen on Optical Coherence Tomography\", <i>Translational Vision Science & Technology</i>, 2022;11(12):3-3.",
        "year": 2022,
        "pub_type": "@article",
        "pub_details": "<i>Translational Vision Science & Technology</i>, 2022;11(12):3-3",
        "bibtex": "Schw22,\n  title={A Deep Learning Framework for the Detection and Quantification of Reticular Pseudodrusen and Drusen on Optical Coherence Tomography},\n  author={Schwartz, Roy and Khalid, Hagar and Liakopoulos, Sandra and Ouyang, Yanling and de Vente, Coen and Gonz{\\'a}lez-Gonzalo, Cristina and Lee, Aaron Y and Guymer, Robyn and Chew, Emily Y and Egan, Catherine and others},\n  journal={Translational Vision Science & Technology}\n  volume={11},\n  number={12},\n  pages={3--3},\n  year={2022},\n  doi={10.1167/tvst.11.12.3},\n  publisher={The Association for Research in Vision and Ophthalmology},\n  scholar_id={15646198235825211737,5612937199194471457}\n"
    },
    "lemi22": {
        "html": "<p>H.G. Lemij, C. de Vente, C.I. S\u00e1nchez, J. Cuadros, N. Jaccard and K. Vermeer. \"Glaucomatous features in fundus photographs of eyes with 'Referable glaucoma' of a large population based labeled data set for training an Artificial Intelligence (AI) algorithm for glaucoma screening\", in: <i>Association for Research in Vision and Ophthalmology</i>, 2022.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-lemi22-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a><a data-ix=\"goupbox\" href=\"https://scholar.google.com/scholar?oi=bibs&hl=nl&cites=14483770842850181670\" target=\"_new\" class=\"knop footerknop movewithmouse w-button publication-button\">Cited by 1</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://iovs.arvojournals.org/article.aspx?articleid=2782322\">URL</a></div>\n",
        "only_text": "H.G. Lemij, C. de Vente, C.I. S\u00e1nchez, J. Cuadros, N. Jaccard and K. Vermeer. \"Glaucomatous features in fundus photographs of eyes with 'Referable glaucoma' of a large population based labeled data set for training an Artificial Intelligence (AI) algorithm for glaucoma screening\", in: <i>Association for Research in Vision and Ophthalmology</i>, 2022.",
        "year": 2022,
        "pub_type": "@conference",
        "pub_details": "in: <i>Association for Research in Vision and Ophthalmology</i>, 2022",
        "bibtex": "Lemi22,\n  title={Glaucomatous features in fundus photographs of eyes with 'Referable glaucoma' of a large population based labeled data set for training an Artificial Intelligence (AI) algorithm for glaucoma screening},\n  author={Lemij, Hans G and de Vente, Coen and S{\\'a}nchez, Clara I and Cuadros, Jorge and Jaccard, Nicolas and Vermeer, Koen},\n  booktitle={Association for Research in Vision and Ophthalmology}\n  volume={63},\n  number={7},\n  url={https://iovs.arvojournals.org/article.aspx?articleid=2782322},\n  pages={2041--A0482},\n  year={2022},\n  publisher={The Association for Research in Vision and Ophthalmology},\n  scholar_id={14483770842850181670},\n"
    },
    "schw22a": {
        "html": "<p>R. Schwartz, H. Khalid, S. Liakopoulos, Y. Ouyang, C. de Vente, C.G. Gonzalo, A.Y. Lee, C.A. Egan, C.I. S\u00e1nchez and A. Tufail. \"A deep learning pipeline for the detection and quantification of drusen and reticular pseudodrusen on optical coherence tomography\", in: <i>Association for Research in Vision and Ophthalmology</i>, 2022.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-schw22a-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://iovs.arvojournals.org/article.aspx?articleid=2781366\">URL</a></div>\n",
        "only_text": "R. Schwartz, H. Khalid, S. Liakopoulos, Y. Ouyang, C. de Vente, C.G. Gonzalo, A.Y. Lee, C.A. Egan, C.I. S\u00e1nchez and A. Tufail. \"A deep learning pipeline for the detection and quantification of drusen and reticular pseudodrusen on optical coherence tomography\", in: <i>Association for Research in Vision and Ophthalmology</i>, 2022.",
        "year": 2022,
        "pub_type": "@conference",
        "pub_details": "in: <i>Association for Research in Vision and Ophthalmology</i>, 2022",
        "bibtex": "Schw22a,\n  title={A deep learning pipeline for the detection and quantification of drusen and reticular pseudodrusen on optical coherence tomography},\n  author={Schwartz, Roy and Khalid, Hagar and Liakopoulos, Sandra and Ouyang, Yanling and de Vente, Coen and Gonzalo, Cristina Gonz{\\'a}lez and Lee, Aaron Y and Egan, Catherine A and S{\\'a}nchez, Clara I and Tufail, Adnan},\n  booktitle={Association for Research in Vision and Ophthalmology}\n  url={https://iovs.arvojournals.org/article.aspx?articleid=2781366},\n  volume={63},\n  number={7},\n  pages={3856--3856},\n  year={2022},\n  publisher={The Association for Research in Vision and Ophthalmology},\n  scholar_id={6469598480820735470},\n"
    },
    "vent22": {
        "html": "<p>C. de Vente, K. Vermeer, N. Jaccard, H.G. Lemij and C.I. S\u00e1nchez. \"AIROGS: Artificial Intelligence for RObust Glaucoma Screening Challenge\", in: <i>Imaging and Morphometry Association for Glaucoma in Europe</i>, 2022.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-vent22-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://drive.google.com/file/d/11DfeyNA8I4UVZGkhn_NVIOIGbcEesfCw/view?usp=sharing\">URL</a></div>\n",
        "only_text": "C. de Vente, K. Vermeer, N. Jaccard, H.G. Lemij and C.I. S\u00e1nchez. \"AIROGS: Artificial Intelligence for RObust Glaucoma Screening Challenge\", in: <i>Imaging and Morphometry Association for Glaucoma in Europe</i>, 2022.",
        "year": 2022,
        "pub_type": "@conference",
        "pub_details": "in: <i>Imaging and Morphometry Association for Glaucoma in Europe</i>, 2022",
        "bibtex": "Vent22,\n  author = {de Vente, Coen and Vermeer, Koen and Jaccard, Nicolas and Lemij, Hans G and S{\\'a}nchez, Clara I},\n  booktitle ={Imaging and Morphometry Association for Glaucoma in Europe}\n  url = {https://drive.google.com/file/d/11DfeyNA8I4UVZGkhn_NVIOIGbcEesfCw/view?usp=sharing},\n  title = {AIROGS: Artificial Intelligence for RObust Glaucoma Screening Challenge},\n\n  Methods: 112,732 CFPs from 60,071 subjects from a population screening program for diabetic retinopathy, obtained from EyePACS, California, USA, were manually labeled by 20 carefully selected and continuously monitored ophthalmologists and optometrists. They each labeled a portion of the full set of images as \u201creferable glaucoma\u201d (RG), \u201cno referable glaucoma\u201d (NRG) or \u201cungradable\u201d (U). Each CFP was graded by 2 randomly selected graders; if their labels matched, it was considered the final label. In case of disagreement, the CFP was graded by a glaucoma specialist; his label was the final label. We split the data into a development set of 101,442 CFPs and a test set of 11,290 CFPs. The challenge task was to classify CFPs as RG or NRG, while additionally providing a decision on whether images were ungradable. To encourage the development of methodologies with inherent robustness mechanisms, we only included CFPs labeled as U in the test set and not in the development set. Challenge participants submitted their solutions as Docker1 containers to our online evaluation platform2. We ran their submitted algorithms on our test set, which is not publicly available. Subsequently, we assessed glaucoma screening performance using the partial area under the receiver operator characteristic curve (pAUC) (90-100% specificity) and sensitivity at 95% specificity (S). To measure robustness, we calculated Cohen's kappa score (\u03baU) and the area under the receiver operator characteristic curve (AUCU), using the decisions generated by the algorithms on image ungradability.\n\n  Results: The challenge is currently running and we are still accepting submissions at the time of writing. Up to now, 289 users have joined the challenge, 208 persons have requested access to the dataset, 26 teams have been formed on the challenge platform, and 13 submissions from 7 unique participants have been successfully submitted to the last preliminary test set, which contains 10% of the test data. The best pAUC, S, \u03baU and AUCU on this preliminary test set were 89.1%, 83.8%, 44.5% and 91.5%, respectively. The means and standard deviations for these metrics over all submissions were 82.2% \u00b1 9.6%, 72.0% \u00b1 19.4%, 20.6% \u00b1 14.9%, 76.8% \u00b1 11.6%, respectively.\n\n  Conclusions: We present a challenge based on real-world data for glaucoma screening by CFP. The initial results are promising, as the performances are high and the preliminary sensitivity at 95% specificity exceeds our target of 80%. The final winners and their solutions will be presented at the 19th IMAGE meeting.\n  },\n  year = {2022},\n"
    },
    "vent23a": {
        "html": "<p>C. de Vente, K.A. Vermeer, N. Jaccard, H. Wang, H. Sun, F. Khader, D. Truhn, T. Aimyshev, Y. Zhanibekuly, T. Le, A. Galdran, M.\u00c1. Gonz\u00e1lez Ballester, G. Carneiro, D.R. G, H.P. S, D. Puthussery, H. Liu, Z. Yang, S. Kondo, S. Kasai, E. Wang, A. Durvasula, J. Heras, M.\u00c1. Zapata, T. Ara\u00fajo, G. Aresta, H. Bogunovi\u0107, M. Arikan, Y.C. Lee, H.B. Cho, Y.H. Choi, A. Qayyum, I. Razzak, B. van Ginneken, H.G. Lemij and C.I. S\u00e1nchez. \"AIROGS: Artificial Intelligence for RObust Glaucoma Screening Challenge\", <i>arXiv:2302.01738</i>, 2023.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-vent23a-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a><a data-ix=\"goupbox\" href=\"https://scholar.google.com/scholar?oi=bibs&hl=nl&cites=16965053761187170060,11379689467925567962,10290591934597328501,12629941101464488854\" target=\"_new\" class=\"knop footerknop movewithmouse w-button publication-button\">Cited by 11</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://doi.org/10.48550/arXiv.2302.01738\">DOI</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://arxiv.org/abs/2302.01738/\">arXiv</a></div>\n",
        "only_text": "C. de Vente, K.A. Vermeer, N. Jaccard, H. Wang, H. Sun, F. Khader, D. Truhn, T. Aimyshev, Y. Zhanibekuly, T. Le, A. Galdran, M.\u00c1. Gonz\u00e1lez Ballester, G. Carneiro, D.R. G, H.P. S, D. Puthussery, H. Liu, Z. Yang, S. Kondo, S. Kasai, E. Wang, A. Durvasula, J. Heras, M.\u00c1. Zapata, T. Ara\u00fajo, G. Aresta, H. Bogunovi\u0107, M. Arikan, Y.C. Lee, H.B. Cho, Y.H. Choi, A. Qayyum, I. Razzak, B. van Ginneken, H.G. Lemij and C.I. S\u00e1nchez. \"AIROGS: Artificial Intelligence for RObust Glaucoma Screening Challenge\", <i>arXiv:2302.01738</i>, 2023.",
        "year": 2023,
        "pub_type": "@Preprint",
        "pub_details": "<i>arXiv:2302.01738</i>, 2023",
        "bibtex": "Vent23a,\n  title={AIROGS: Artificial Intelligence for RObust Glaucoma Screening Challenge},\n  author={de Vente, Coen and Vermeer, Koenraad A. and Jaccard, Nicolas and Wang, He and Sun, Hongyi and Khader, Firas and Truhn, Daniel and Aimyshev, Temirgali and Zhanibekuly, Yerkebulan and Le, Tien-Dung and Galdran, Adrian and Gonz\u00e1lez Ballester, Miguel \u00c1ngel and Carneiro, Gustavo and G, Devika R and S, Hrishikesh P and Puthussery, Densen and Liu, Hong and Yang, Zekang and Kondo, Satoshi and Kasai, Satoshi and Wang, Edward and Durvasula, Ashritha and Heras, J\u00f3nathan and Zapata, Miguel \u00c1ngel and Ara\u00fajo, Teresa and Aresta, Guilherme and Bogunovi\u0107, Hrvoje and Arikan, Mustafa and Lee, Yeong Chan and Cho, Hyun Bin and Choi, Yoon Ho and Qayyum, Abdul and Razzak, Imran and van Ginneken, Bram and Lemij, Hans G. and S\u00e1nchez, Clara I.},\n  journal={arXiv:2302.01738}\n  year={2023},\n  scholar_id={16965053761187170060,11379689467925567962,10290591934597328501,12629941101464488854},\n  doi={10.48550/arXiv.2302.01738}\n"
    },
    "vent23b": {
        "html": "<p>C. de Vente, B. van Ginneken, C.B. Hoyng, C.C. Klaver and C.I. S\u00e1nchez. \"Uncertainty-Aware Multiple-Instance Learning for Reliable Classification: Application to Optical Coherence Tomography\", <i>arXiv:2302.03116</i>, 2023.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-vent23b-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://doi.org/10.48550/arXiv.2302.03116\">DOI</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://arxiv.org/abs/2302.03116/\">arXiv</a></div>\n",
        "only_text": "C. de Vente, B. van Ginneken, C.B. Hoyng, C.C. Klaver and C.I. S\u00e1nchez. \"Uncertainty-Aware Multiple-Instance Learning for Reliable Classification: Application to Optical Coherence Tomography\", <i>arXiv:2302.03116</i>, 2023.",
        "year": 2023,
        "pub_type": "@Preprint",
        "pub_details": "<i>arXiv:2302.03116</i>, 2023",
        "bibtex": "Vent23b,\n  title={Uncertainty-Aware Multiple-Instance Learning for Reliable Classification: Application to Optical Coherence Tomography},\n  author={de Vente, Coen and van Ginneken, Bram and Hoyng, Carel B and Klaver, Caroline CW and S{\\'a}nchez, Clara I},\n  journal={arXiv:2302.03116}\n  year={2023},\n  doi={10.48550/arXiv.2302.03116}\n"
    },
    "lemi23": {
        "html": "<p>H.G. Lemij, C. de Vente, C.I. S\u00e1nchez and K.A. Vermeer. \"Characteristics of a large, labeled dataset for the training of artificial intelligence for glaucoma screening with fundus photographs\", <i>Ophthalmology Science</i>, 2023;100300.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-lemi23-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://doi.org/10.1016/j.xops.2023.100300\">DOI</a></div>\n",
        "only_text": "H.G. Lemij, C. de Vente, C.I. S\u00e1nchez and K.A. Vermeer. \"Characteristics of a large, labeled dataset for the training of artificial intelligence for glaucoma screening with fundus photographs\", <i>Ophthalmology Science</i>, 2023;100300.",
        "year": 2023,
        "pub_type": "@article",
        "pub_details": "<i>Ophthalmology Science</i>, 2023;100300",
        "bibtex": "Lemi23,\n  title={Characteristics of a large, labeled dataset for the training of artificial intelligence for glaucoma screening with fundus photographs},\n  author={Lemij, Hans G and de Vente, Coen and S{\\'a}nchez, Clara I and Vermeer, Koen A},\n  journal={Ophthalmology Science}\n  pages={100300},\n  year={2023},\n  publisher={Elsevier},\n  doi={10.1016/j.xops.2023.100300}\n"
    },
    "vent23c": {
        "html": "<p>C. de Vente, A. Tufail, S. Schmitz-Valckenberg, M. Sa\u00dfmannshausen, C. Hoyng and C.I. S\u00e1nchez on behalf of the MACUSTAR consortium. \"OCT Super-Resolution for Data Standardization using AI: A MACUSTAR report\", in: <i>Association for Research in Vision and Ophthalmology</i>, 2023.</p><div class=\"publication-button-group\"><a data-ix=\"goupbox\" id=\"publication-modal-vent23c-button\" class=\"knop footerknop movewithmouse w-button publication-button\">Cite</a> <a data-ix=\"goupbox\" target=\"_blank\" class=\"knop footerknop movewithmouse w-button publication-button\" href=\"https://iovs.arvojournals.org/article.aspx?articleid=2789903\">URL</a></div>\n",
        "only_text": "C. de Vente, A. Tufail, S. Schmitz-Valckenberg, M. Sa\u00dfmannshausen, C. Hoyng and C.I. S\u00e1nchez on behalf of the MACUSTAR consortium. \"OCT Super-Resolution for Data Standardization using AI: A MACUSTAR report\", in: <i>Association for Research in Vision and Ophthalmology</i>, 2023.",
        "year": 2023,
        "pub_type": "@conference",
        "pub_details": "in: <i>Association for Research in Vision and Ophthalmology</i>, 2023",
        "bibtex": "Vent23c,\n  author = {de Vente, Coen and Tufail, A. and Schmitz-Valckenberg, S. and Sa\u00dfmannshausen, M. and Hoyng, C. and S{\\'a}nchez on behalf of the MACUSTAR consortium, Clara I},\n  title = {OCT Super-Resolution for Data Standardization using AI: A MACUSTAR report},\n\nMethods:  The MACUSTAR cohort, a European multicentre study, was used as a training set with 743 OCTs from 181 patients and validation set with 26 OCTs from 26 patients (n=3 no AMD, n=2 early AMD, n=18 intermediate AMD, n=3 late AMD). All scans were Heidelberg Spectralis OCTs with 241 B-scans. We trained a 3D diffusion model to generate high-resolution OCTs, which was used during evaluation to produce OCTs with 241 B-scans from OCTs with 120 B-scans. The performance was calculated using the mean squared error (MSE) on OCT volume-level between the generated B-scans and the original B-scans.\n\nResults: The MSE between the generated B-scans from the low-resolution OCTs and the original B-scans from the high-resolution OCTs was 0.006 \u00b1 0.004 (mean \u00b1 SD). Fig. 1 shows visual examples of the generated OCTs compared to the original B-scans in the validation set.\n\nConclusions:  We showed the feasibility of the proposed approach to generate super-resolution OCTs, which is one of the required steps to standardize high-quality OCTs within multicenter studies. In extensions of this approach, coherence between the OCT and other modalities, such as en face imaging and other metadata, could be introduced, allowing the AI model to make better informed generative decisions.\n},\n  year = {2023},\n  booktitle={Association for Research in Vision and Ophthalmology}\n  journal={Investigative Ophthalmology & Visual Science}\n  volume={64},\n  number={8},\n  pages={313--313},\n  publisher={The Association for Research in Vision and Ophthalmology},\n  url={https://iovs.arvojournals.org/article.aspx?articleid=2789903},\n"
    }
}